{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MIDAS Heart/Resp Motion Pipeline\n",
        "\n",
        "This notebook separates respiratory and cardiac motion, detects beats, and builds\n",
        "time-series features for grouping into four conditions (control, doxo, doxo+epa, other).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup\n",
        "Load data from `./data`, estimate sampling rate, and define parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from scipy.signal import butter, sosfiltfilt, welch, find_peaks, hilbert\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.model_selection import GroupKFold, cross_val_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "try:\n",
        "    from sktime.transformations.panel.rocket import MiniRocket\n",
        "except Exception:\n",
        "    MiniRocket = None\n",
        "\n",
        "try:\n",
        "    import hdbscan\n",
        "except Exception:\n",
        "    hdbscan = None\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "DATA_DIR = Path(\"../data\")\n",
        "RESP_BAND_HZ = (0.1, 2.0)\n",
        "HEART_BAND_HZ = (2.5, 15.0)\n",
        "REFRACTORY_S = 0.20\n",
        "BEAT_WINDOW_S = (0.15, 0.25)\n",
        "RESAMPLE_LEN = 128\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load data and estimate sampling rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Record:\n",
        "    path: Path\n",
        "    label: str\n",
        "    time_s: np.ndarray\n",
        "    signal: np.ndarray\n",
        "    fs: float\n",
        "    group_id: str\n",
        "\n",
        "\n",
        "def label_from_name(name: str) -> str:\n",
        "    lower = name.lower()\n",
        "    if lower.startswith(\"control\"):\n",
        "        return \"control\"\n",
        "    if lower.startswith(\"doxo\"):\n",
        "        return \"doxo\"\n",
        "    if lower.startswith(\"empa_doxo\") or lower.startswith(\"preconditionare_empa_doxo\"):\n",
        "        return \"empa_doxo\"\n",
        "    if lower.startswith(\"empa\"):\n",
        "        return \"empa\"\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "def load_records(data_dir: Path) -> list[Record]:\n",
        "    records: list[Record] = []\n",
        "    for path in sorted(data_dir.glob(\"*.csv\")):\n",
        "        df = pd.read_csv(path)\n",
        "        time_s = df.iloc[:, 0].to_numpy(dtype=float)\n",
        "        signal = df.iloc[:, 1].to_numpy(dtype=float)\n",
        "        dt = np.diff(time_s)\n",
        "        fs = 1.0 / float(np.median(dt)) if len(dt) else 0.0\n",
        "        records.append(\n",
        "            Record(\n",
        "                path=path,\n",
        "                label=label_from_name(path.stem),\n",
        "                time_s=time_s,\n",
        "                signal=signal,\n",
        "                fs=fs,\n",
        "                group_id=path.stem,\n",
        "            )\n",
        "        )\n",
        "    return records\n",
        "\n",
        "\n",
        "records = load_records(DATA_DIR)\n",
        "{rec.path.name: rec.fs for rec in records}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Spectrum inspection (Welch PSD)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rec = records[0]\n",
        "f, pxx = welch(rec.signal, fs=rec.fs, nperseg=min(2048, len(rec.signal)))\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.semilogy(f, pxx)\n",
        "plt.title(f\"PSD: {rec.path.name}\")\n",
        "plt.xlabel(\"Hz\")\n",
        "plt.ylabel(\"Power\")\n",
        "plt.xlim(0, 20)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Separate respiration and heart (zero-phase filtering)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def band_limits(band: tuple[float, float], fs: float) -> tuple[float, float]:\n",
        "    low, high = band\n",
        "    nyq = fs / 2.0\n",
        "    return (max(0.01, low), min(high, 0.49 * nyq))\n",
        "\n",
        "\n",
        "def butter_sos(band: tuple[float, float], fs: float, order: int = 4) -> np.ndarray:\n",
        "    low, high = band_limits(band, fs)\n",
        "    return butter(order, [low, high], btype=\"bandpass\", fs=fs, output=\"sos\")\n",
        "\n",
        "\n",
        "def lowpass_sos(cutoff: float, fs: float, order: int = 4) -> np.ndarray:\n",
        "    cutoff = min(cutoff, 0.49 * (fs / 2.0))\n",
        "    return butter(order, cutoff, btype=\"lowpass\", fs=fs, output=\"sos\")\n",
        "\n",
        "\n",
        "def separate_components(signal: np.ndarray, fs: float) -> tuple[np.ndarray, np.ndarray]:\n",
        "    resp_sos = lowpass_sos(RESP_BAND_HZ[1], fs)\n",
        "    heart_sos = butter_sos(HEART_BAND_HZ, fs)\n",
        "    resp = sosfiltfilt(resp_sos, signal)\n",
        "    heart = sosfiltfilt(heart_sos, signal)\n",
        "    return resp, heart\n",
        "\n",
        "\n",
        "resp, heart = separate_components(rec.signal, rec.fs)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(rec.time_s, rec.signal, label=\"raw\", alpha=0.5)\n",
        "plt.plot(rec.time_s, resp, label=\"resp\")\n",
        "plt.plot(rec.time_s, heart, label=\"heart\")\n",
        "plt.legend()\n",
        "plt.title(f\"Separated components: {rec.path.name}\")\n",
        "plt.xlabel(\"Time (s)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Beat detection on heart component\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def heart_envelope(heart: np.ndarray) -> np.ndarray:\n",
        "    return np.abs(hilbert(heart))\n",
        "\n",
        "\n",
        "def detect_beats(heart: np.ndarray, fs: float) -> tuple[np.ndarray, np.ndarray]:\n",
        "    env = heart_envelope(heart)\n",
        "    distance = max(1, int(REFRACTORY_S * fs))\n",
        "    prominence = np.percentile(env, 75) * 0.5\n",
        "    peaks, _ = find_peaks(env, distance=distance, prominence=prominence)\n",
        "    return peaks, env\n",
        "\n",
        "\n",
        "peaks, env = detect_beats(heart, rec.fs)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(rec.time_s, env, label=\"envelope\")\n",
        "plt.plot(rec.time_s[peaks], env[peaks], \"rx\", label=\"beats\")\n",
        "plt.legend()\n",
        "plt.title(f\"Beat detection: {rec.path.name}\")\n",
        "plt.xlabel(\"Time (s)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Extract beat windows (fixed length)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def extract_beat_windows(\n",
        "    signal: np.ndarray,\n",
        "    peaks: np.ndarray,\n",
        "    fs: float,\n",
        "    window_s: tuple[float, float],\n",
        "    resample_len: int,\n",
        ") -> list[np.ndarray]:\n",
        "    pre_s, post_s = window_s\n",
        "    pre = int(pre_s * fs)\n",
        "    post = int(post_s * fs)\n",
        "    windows: list[np.ndarray] = []\n",
        "    for peak in peaks:\n",
        "        start = peak - pre\n",
        "        end = peak + post\n",
        "        if start < 0 or end >= len(signal):\n",
        "            continue\n",
        "        snippet = signal[start:end]\n",
        "        x_old = np.linspace(0.0, 1.0, num=len(snippet), endpoint=False)\n",
        "        x_new = np.linspace(0.0, 1.0, num=resample_len, endpoint=False)\n",
        "        windows.append(np.interp(x_new, x_old, snippet))\n",
        "    return windows\n",
        "\n",
        "\n",
        "beat_windows = extract_beat_windows(heart, peaks, rec.fs, BEAT_WINDOW_S, RESAMPLE_LEN)\n",
        "len(beat_windows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ground-truth groups (from filenames)\n",
        "\n",
        "Labels are derived from filename prefixes only and are used **only** for evaluation. Clustering models do not see these labels during fitting. The four target groups are:\n",
        "\n",
        "- control\n",
        "- doxo (including doxo_re)\n",
        "- empa\n",
        "- empa_doxo (including preconditionare_empa_doxo)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Build dataset (beats -> features -> grouping)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "beats: list[np.ndarray] = []\n",
        "labels: list[str] = []\n",
        "groups: list[str] = []\n",
        "\n",
        "for rec in records:\n",
        "    _, heart = separate_components(rec.signal, rec.fs)\n",
        "    peaks, _ = detect_beats(heart, rec.fs)\n",
        "    windows = extract_beat_windows(heart, peaks, rec.fs, BEAT_WINDOW_S, RESAMPLE_LEN)\n",
        "    beats.extend(windows)\n",
        "    labels.extend([rec.label] * len(windows))\n",
        "    groups.extend([rec.group_id] * len(windows))\n",
        "\n",
        "X = np.stack(beats) if beats else np.empty((0, RESAMPLE_LEN))\n",
        "y = np.array(labels)\n",
        "group_ids = np.array(groups)\n",
        "X.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) MiniROCKET baseline (supervised)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if MiniRocket is None or len(X) == 0:\n",
        "    print(\"MiniRocket not available or no beats extracted.\")\n",
        "else:\n",
        "    X3d = X[:, np.newaxis, :]\n",
        "    rocket = MiniRocket()\n",
        "    X_feat = rocket.fit_transform(X3d)\n",
        "    clf = RidgeClassifierCV(alphas=np.logspace(-3, 3, 7))\n",
        "    unique_groups = np.unique(group_ids)\n",
        "    if len(unique_groups) >= 3:\n",
        "        splits = GroupKFold(n_splits=min(5, len(unique_groups))).split(X_feat, y, group_ids)\n",
        "        scores = cross_val_score(clf, X_feat, y, cv=splits)\n",
        "        print(f\"Group CV accuracy: {scores.mean():.3f} +/- {scores.std():.3f}\")\n",
        "    else:\n",
        "        clf.fit(X_feat, y)\n",
        "        print(\"Trained on full data (insufficient groups for CV).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Clustering options (exploratory)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate_clustering(y_true: np.ndarray, y_pred: np.ndarray, label: str, ignore_noise: bool = False) -> None:\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    if ignore_noise and np.any(y_pred == -1):\n",
        "        mask = y_pred != -1\n",
        "        print(f\"{label} noise fraction: {1 - mask.mean():.2%}\")\n",
        "        y_true = y_true[mask]\n",
        "        y_pred = y_pred[mask]\n",
        "    if len(y_true) == 0 or len(np.unique(y_pred)) < 2:\n",
        "        print(f\"{label}: insufficient clusters for evaluation.\")\n",
        "        return\n",
        "    ari = adjusted_rand_score(y_true, y_pred)\n",
        "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
        "    print(f\"{label} ARI: {ari:.3f} | NMI: {nmi:.3f}\")\n",
        "\n",
        "    true_labels = sorted(set(y_true))\n",
        "    pred_labels = sorted(set(y_pred))\n",
        "    true_map = {lab: idx for idx, lab in enumerate(true_labels)}\n",
        "    pred_map = {lab: idx for idx, lab in enumerate(pred_labels)}\n",
        "    contingency = np.zeros((len(true_labels), len(pred_labels)), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        contingency[true_map[t], pred_map[p]] += 1\n",
        "    row_ind, col_ind = linear_sum_assignment(-contingency)\n",
        "    matched = contingency[row_ind, col_ind].sum()\n",
        "    best_acc = matched / len(y_true)\n",
        "    mapping = {pred_labels[c]: true_labels[r] for r, c in zip(row_ind, col_ind)}\n",
        "    print(f\"{label} best alignment accuracy: {best_acc:.3f}\")\n",
        "    print(f\"{label} mapping (cluster -> label): {mapping}\")\n",
        "\n",
        "\n",
        "if MiniRocket is None or len(X) == 0:\n",
        "    print(\"MiniRocket not available or no beats extracted.\")\n",
        "else:\n",
        "    X3d = X[:, np.newaxis, :]\n",
        "    rocket = MiniRocket()\n",
        "    X_feat = rocket.fit_transform(X3d)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=4, n_init=20, random_state=0)\n",
        "    clusters = kmeans.fit_predict(X_feat)\n",
        "    unique, counts = np.unique(clusters, return_counts=True)\n",
        "    print(\"KMeans cluster counts\", dict(zip(unique, counts)))\n",
        "    evaluate_clustering(y, clusters, label=\"KMeans\")\n",
        "\n",
        "    if hdbscan is not None:\n",
        "        clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
        "        hdb = clusterer.fit_predict(X_feat)\n",
        "        unique, counts = np.unique(hdb, return_counts=True)\n",
        "        print(\"HDBSCAN cluster counts\", dict(zip(unique, counts)))\n",
        "        evaluate_clustering(y, hdb, label=\"HDBSCAN\", ignore_noise=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}